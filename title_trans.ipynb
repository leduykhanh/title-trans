{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shopee-textblob.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-okiAef34aT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e6e11a80-147c-4c6c-fbeb-529eec5fa8b5"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import textblob\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmucPmvF4fcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "dc8c9d57-b11b-4a53-e972-960e92598683"
      },
      "source": [
        "!unzip '/content/drive/My Drive/shopee-titles/shopee-product-title-translation-open.zip' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/shopee-titles/shopee-product-title-translation-open.zip\n",
            "  inflating: dev_en.csv              \n",
            "  inflating: dev_tcn.csv             \n",
            "  inflating: test_tcn.csv            \n",
            "  inflating: train_en.csv            \n",
            "  inflating: train_tcn.csv           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WRtxxg_8nqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6aa5497d-c93d-4614-ccfe-2b988fed9b6c"
      },
      "source": [
        "%%bash\n",
        "pip install jieba\n",
        "pip install fasttext\n",
        "pip install hanziconv\n",
        "pip install fairseq\n",
        "pip install sacrebleu\n",
        "pip install googletrans\n",
        "\n",
        "cd /usr/local/lib/python3.6/dist-packages/fairseq/modules/lightconv_layer\n",
        "cd ..\n",
        "wget -L https://raw.githubusercontent.com/pytorch/fairseq/master/fairseq/modules/cuda_utils.cu\n",
        "cd lightconv_layer\n",
        "wget -L https://raw.githubusercontent.com/pytorch/fairseq/master/fairseq/modules/lightconv_layer/lightconv_cuda.cpp\n",
        "wget -L https://raw.githubusercontent.com/pytorch/fairseq/master/fairseq/modules/lightconv_layer/lightconv_cuda.cuh\n",
        "wget -L https://raw.githubusercontent.com/pytorch/fairseq/master/fairseq/modules/lightconv_layer/lightconv_cuda_kernel.cu\n",
        "python cuda_function_gen.py\n",
        "python setup.py install\n",
        "\n",
        "\n",
        "cd /usr/local/lib/python3.6/dist-packages/fairseq/modules/dynamicconv_layer\n",
        "wget -L https://raw.githubusercontent.com/pytorch/fairseq/master/fairseq/modules/dynamicconv_layer/dynamicconv_cuda.cpp\n",
        "wget -L https://raw.githubusercontent.com/pytorch/fairseq/master/fairseq/modules/dynamicconv_layer/dynamicconv_cuda.cuh\n",
        "wget -L https://raw.githubusercontent.com/pytorch/fairseq/master/fairseq/modules/dynamicconv_layer/dynamicconv_cuda_kernel.cu\n",
        "python cuda_function_gen.py\n",
        "python setup.py install\n",
        "pip install sacremoses subword_nmt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (0.42.1)\n",
            "Collecting fasttext\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (49.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py): started\n",
            "  Building wheel for fasttext (setup.py): finished with status 'done'\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3015251 sha256=1b9250be081f59c11edc889a199c33713cd308fcfe22897c24e376ac7716de83\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n",
            "Collecting hanziconv\n",
            "  Downloading https://files.pythonhosted.org/packages/63/71/b89cb63077fd807fe31cf7c016a06e7e579a289d8a37aa24a30282d02dd2/hanziconv-0.3.2.tar.gz (276kB)\n",
            "Building wheels for collected packages: hanziconv\n",
            "  Building wheel for hanziconv (setup.py): started\n",
            "  Building wheel for hanziconv (setup.py): finished with status 'done'\n",
            "  Created wheel for hanziconv: filename=hanziconv-0.3.2-py2.py3-none-any.whl size=23215 sha256=1041d98df63ca924e8760979deaf70fc4e74b4882e826a79c897821fcb745518\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/d8/3c/c39898fa9c9ce6e34b0ab4c6604892462d440c743715c94054\n",
            "Successfully built hanziconv\n",
            "Installing collected packages: hanziconv\n",
            "Successfully installed hanziconv-0.3.2\n",
            "Collecting fairseq\n",
            "  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/23/d3/be980ad7cda7c4bbfa97ee3de062fb3014fc1a34d6dd5b82d7b92f8d6522/sacrebleu-1.4.13-py3-none-any.whl (43kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/e7/ceef002a300a98a208232fab593183249b6964b306ee7dabb29908419cca/portalocker-1.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py): started\n",
            "  Building wheel for fairseq (setup.py): finished with status 'done'\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2021665 sha256=d95779ce81617df3b17d4f402b11f9407a73a450bef02280fa163ffaf0a0af77\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq\n",
            "Successfully installed fairseq-0.9.0 portalocker-1.7.1 sacrebleu-1.4.13\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.4.13)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (1.7.1)\n",
            "Collecting googletrans\n",
            "  Downloading https://files.pythonhosted.org/packages/71/3a/3b19effdd4c03958b90f40fe01c93de6d5280e03843cc5adf6956bfc9512/googletrans-3.0.0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Collecting hstspreload\n",
            "  Downloading https://files.pythonhosted.org/packages/55/8b/5adb4f33ec2555122b15fdbf6c7fc5d59dafde7f0d3289adb5dee6843ad3/hstspreload-2020.7.29-py3-none-any.whl (926kB)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/82/4bd4b7d9c0d1dc0fbfbc2a1e00138e7f3ab85bc239358fe9b78aa2ab586d/sniffio-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.6.20)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Collecting h2==3.*\n",
            "  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "Collecting h11<0.10,>=0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "Collecting contextvars>=2.1; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting immutables>=0.9\n",
            "  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "Building wheels for collected packages: googletrans, contextvars\n",
            "  Building wheel for googletrans (setup.py): started\n",
            "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-cp36-none-any.whl size=15736 sha256=6b0dd54321a5b78c1d8bae8bf4d08c11413d7686a7d7a782322e80d36a7a8903\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/1a/a7/eaf4d7a3417a0c65796c547cff4deb6d79c7d14c2abd29273e\n",
            "  Building wheel for contextvars (setup.py): started\n",
            "  Building wheel for contextvars (setup.py): finished with status 'done'\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=0a43d6f0194a57b274bbfe72bb7e09a2c33512b5569214d7f1961791b691391a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built googletrans contextvars\n",
            "Installing collected packages: hstspreload, rfc3986, immutables, contextvars, sniffio, hyperframe, hpack, h2, h11, httpcore, httpx, googletrans\n",
            "Successfully installed contextvars-2.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.7.29 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.14 rfc3986-1.4.0 sniffio-1.1.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating lightconv_layer.egg-info\n",
            "writing lightconv_layer.egg-info/PKG-INFO\n",
            "writing dependency_links to lightconv_layer.egg-info/dependency_links.txt\n",
            "writing top-level names to lightconv_layer.egg-info/top_level.txt\n",
            "writing manifest file 'lightconv_layer.egg-info/SOURCES.txt'\n",
            "reading manifest file 'lightconv_layer.egg-info/SOURCES.txt'\n",
            "writing manifest file 'lightconv_layer.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'lightconv_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c lightconv_cuda.cpp -o build/temp.linux-x86_64-3.6/lightconv_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lightconv_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c lightconv_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/lightconv_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lightconv_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/lightconv_cuda.o build/temp.linux-x86_64-3.6/lightconv_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/lightconv_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/lightconv_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for lightconv_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/lightconv_cuda.py to lightconv_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying lightconv_layer.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying lightconv_layer.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying lightconv_layer.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying lightconv_layer.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "creating dist\n",
            "creating 'dist/lightconv_layer-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing lightconv_layer-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/lightconv_layer-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting lightconv_layer-0.0.0-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding lightconv-layer 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/lightconv_layer-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for lightconv-layer==0.0.0\n",
            "Finished processing dependencies for lightconv-layer==0.0.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating dynamicconv_layer.egg-info\n",
            "writing dynamicconv_layer.egg-info/PKG-INFO\n",
            "writing dependency_links to dynamicconv_layer.egg-info/dependency_links.txt\n",
            "writing top-level names to dynamicconv_layer.egg-info/top_level.txt\n",
            "writing manifest file 'dynamicconv_layer.egg-info/SOURCES.txt'\n",
            "reading manifest file 'dynamicconv_layer.egg-info/SOURCES.txt'\n",
            "writing manifest file 'dynamicconv_layer.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'dynamicconv_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c dynamicconv_cuda.cpp -o build/temp.linux-x86_64-3.6/dynamicconv_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=dynamicconv_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c dynamicconv_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/dynamicconv_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=dynamicconv_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/dynamicconv_cuda.o build/temp.linux-x86_64-3.6/dynamicconv_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/dynamicconv_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/dynamicconv_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for dynamicconv_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/dynamicconv_cuda.py to dynamicconv_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicconv_layer.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicconv_layer.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicconv_layer.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dynamicconv_layer.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "creating dist\n",
            "creating 'dist/dynamicconv_layer-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing dynamicconv_layer-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/dynamicconv_layer-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting dynamicconv_layer-0.0.0-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding dynamicconv-layer 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/dynamicconv_layer-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for dynamicconv-layer==0.0.0\n",
            "Finished processing dependencies for dynamicconv-layer==0.0.0\n",
            "Collecting sacremoses\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "Collecting subword_nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.41.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py): started\n",
            "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=70dece4ff167ad5aa3aeaaa095b12f017deca490d105d983ceac84de5e47f68e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, subword-nmt\n",
            "Successfully installed sacremoses-0.0.43 subword-nmt-0.3.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ0lscv44jbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d77658d4-8767-4165-f665-9a17f8b617cd"
      },
      "source": [
        "dfs = {}\n",
        "\n",
        "for fn in ['train_en', 'train_tcn']:\n",
        "    lines = []\n",
        "    cats = []\n",
        "    with open(f'{fn}.csv', 'r') as f:\n",
        "        for line in f: \n",
        "            break\n",
        "        for line in f:\n",
        "            if line.count(',') == 0:\n",
        "                continue\n",
        "            phrases = line.strip().split(',')\n",
        "            cats.append(phrases[-1])\n",
        "            line = ','.join(phrases[:-1])\n",
        "            lines.append(','.join(phrases[:-1]))\n",
        "\n",
        "    # with open(f'{fn}_cleaned.csv', 'w') as f:\n",
        "    #     f.writelines(lines)\n",
        "\n",
        "    dfs[f'{fn}_cleaned'] = pd.DataFrame({'product_title': lines, 'category': cats})\n",
        "    \n",
        "\n",
        "files = ['dev_en', 'dev_tcn', 'test_tcn']\n",
        "for f in files:\n",
        "    print(f)\n",
        "    dfs[f] = pd.read_csv(f'{f}.csv', escapechar='\\\\')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev_en\n",
            "dev_tcn\n",
            "test_tcn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3km78XStChXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a42fb804-a050-4685-8fd1-fbab5dba5cac"
      },
      "source": [
        "import torch\n",
        "\n",
        "# List available models\n",
        "torch.hub.list('pytorch/fairseq')  # [..., 'lightconv.glu.wmt17.zh-en', ... ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/fairseq/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:305: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/fairseq\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/fairseq\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.data_utils_fast' extension\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/data\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/data\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.token_block_utils_fast' extension\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'fairseq.libnat' extension\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib/libnat\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so -> fairseq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bart.base',\n",
              " 'bart.large',\n",
              " 'bart.large.cnn',\n",
              " 'bart.large.mnli',\n",
              " 'bart.large.xsum',\n",
              " 'bpe',\n",
              " 'camembert',\n",
              " 'camembert-base',\n",
              " 'camembert-base-ccnet',\n",
              " 'camembert-base-ccnet-4gb',\n",
              " 'camembert-base-oscar-4gb',\n",
              " 'camembert-base-wikipedia-4gb',\n",
              " 'camembert-large',\n",
              " 'camembert.v0',\n",
              " 'conv.stories',\n",
              " 'conv.stories.pretrained',\n",
              " 'conv.wmt14.en-de',\n",
              " 'conv.wmt14.en-fr',\n",
              " 'conv.wmt17.en-de',\n",
              " 'data.stories',\n",
              " 'dynamicconv.glu.wmt14.en-fr',\n",
              " 'dynamicconv.glu.wmt16.en-de',\n",
              " 'dynamicconv.glu.wmt17.en-de',\n",
              " 'dynamicconv.glu.wmt17.zh-en',\n",
              " 'dynamicconv.no_glu.iwslt14.de-en',\n",
              " 'dynamicconv.no_glu.wmt16.en-de',\n",
              " 'lightconv.glu.wmt14.en-fr',\n",
              " 'lightconv.glu.wmt16.en-de',\n",
              " 'lightconv.glu.wmt17.en-de',\n",
              " 'lightconv.glu.wmt17.zh-en',\n",
              " 'lightconv.no_glu.iwslt14.de-en',\n",
              " 'lightconv.no_glu.wmt16.en-de',\n",
              " 'roberta.base',\n",
              " 'roberta.large',\n",
              " 'roberta.large.mnli',\n",
              " 'roberta.large.wsc',\n",
              " 'tokenizer',\n",
              " 'transformer.wmt14.en-fr',\n",
              " 'transformer.wmt16.en-de',\n",
              " 'transformer.wmt18.en-de',\n",
              " 'transformer.wmt19.de-en',\n",
              " 'transformer.wmt19.de-en.single_model',\n",
              " 'transformer.wmt19.en-de',\n",
              " 'transformer.wmt19.en-de.single_model',\n",
              " 'transformer.wmt19.en-ru',\n",
              " 'transformer.wmt19.en-ru.single_model',\n",
              " 'transformer.wmt19.ru-en',\n",
              " 'transformer.wmt19.ru-en.single_model',\n",
              " 'transformer_lm.gbw.adaptive_huge',\n",
              " 'transformer_lm.wiki103.adaptive',\n",
              " 'transformer_lm.wmt19.de',\n",
              " 'transformer_lm.wmt19.en',\n",
              " 'transformer_lm.wmt19.ru',\n",
              " 'xlmr.base',\n",
              " 'xlmr.large']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB0hlFhifryk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "50f2f248-d52b-4ab2-b2c1-abdd4d882c2c"
      },
      "source": [
        "dfs['dev_en']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>translation_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oppo A75 A75S A73 Phone Case Soft Rabbit Silic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SOFT 99 Coating Car Wax Strong Water Watt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Low Sugar Mango Dry 250g Be The Royal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>* the culture Japan Imported Round Top Space C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello Kitty Sandals Shoes White/Red Children n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Hippored Torn Fun Unique Style Straight Jeans ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Kids Set Table Bay - Thin Long Sleeve Home Sui...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>LONGCHAMP Le Pliage Neo High Density Nylon Bac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>IFairies Opening Adjustable Ring ifairies [564...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>PolarStar Women Sweat Quick Dry T-shirt Black ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    translation_output\n",
              "0    Oppo A75 A75S A73 Phone Case Soft Rabbit Silic...\n",
              "1            SOFT 99 Coating Car Wax Strong Water Watt\n",
              "2                Low Sugar Mango Dry 250g Be The Royal\n",
              "3    * the culture Japan Imported Round Top Space C...\n",
              "4    Hello Kitty Sandals Shoes White/Red Children n...\n",
              "..                                                 ...\n",
              "995  Hippored Torn Fun Unique Style Straight Jeans ...\n",
              "996  Kids Set Table Bay - Thin Long Sleeve Home Sui...\n",
              "997  LONGCHAMP Le Pliage Neo High Density Nylon Bac...\n",
              "998  IFairies Opening Adjustable Ring ifairies [564...\n",
              "999  PolarStar Women Sweat Quick Dry T-shirt Black ...\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaBID7CsfguO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('dev_zh_cleaned.txt', 'w') as f:\n",
        "    for line in dfs['dev_tcn'].text.values:\n",
        "        f.write(process_tcn(line) + '\\n')\n",
        "\n",
        "with open('dev_en_cleaned.txt', 'w') as f:\n",
        "    for line in dfs['dev_en'].translation_output.values:\n",
        "        f.write(line + '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPVl_9K8KaF6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "fc19f3d2-feee-4fb4-fdec-d4e8caa7c54c"
      },
      "source": [
        "import fairseq\n",
        "import jieba\n",
        "from hanziconv import HanziConv\n",
        "import sacrebleu\n",
        "\n",
        "# https://github.com/pytorch/fairseq/tree/master/examples/pay_less_attention_paper\n",
        "\n",
        "zh2en = torch.hub.load('pytorch/fairseq', 'lightconv.glu.wmt17.zh-en', tokenizer='moses', bpe='subword_nmt')\n",
        "\n",
        "zh2en.cuda()\n",
        "\n",
        "# The underlying model is available under the *models* attribute\n",
        "assert isinstance(zh2en.models[0], fairseq.models.lightconv.LightConvModel)\n",
        "\n",
        "\n",
        "# Translate a sentence\n",
        "zh2en.translate('你好 世界')\n",
        "# 'Hello World'\n",
        "# zh2endyn = torch.hub.load('pytorch/fairseq', 'dynamicconv.glu.wmt17.zh-en', tokenizer='moses', bpe='subword_nmt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n",
            "100%|██████████| 2998309912/2998309912 [03:53<00:00, 12821119.26B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n",
            "No module named 'lightconv_cuda'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello World'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfDatMZq2RpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "6e6bcac4-b791-49c3-ffb1-327b2019ac2f"
      },
      "source": [
        "zh2endyn = torch.hub.load('pytorch/fairseq', 'dynamicconv.glu.wmt17.zh-en', tokenizer='moses', bpe='subword_nmt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n",
            "100%|██████████| 2994863428/2994863428 [03:52<00:00, 12873246.09B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n",
            "No module named 'dynamicconv_cuda'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MzLg-xR3WDb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8652ac49-6028-4b9d-8138-6416a46e905d"
      },
      "source": [
        "zh2endyn.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorHubInterface(\n",
              "  (models): ModuleList(\n",
              "    (0): LightConvModel(\n",
              "      (encoder): LightConvEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(48216, 1024, padding_idx=1)\n",
              "        (embed_positions): SinusoidalPositionalEmbedding()\n",
              "        (layers): ModuleList(\n",
              "          (0): LightConvEncoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=3, padding_l=1, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=48, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (layer_norms): ModuleList(\n",
              "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (1): LightConvEncoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=7, padding_l=3, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=112, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (layer_norms): ModuleList(\n",
              "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (2): LightConvEncoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=15, padding_l=7, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=240, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (layer_norms): ModuleList(\n",
              "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (3): LightConvEncoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=31, padding_l=15, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=496, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (layer_norms): ModuleList(\n",
              "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (4): LightConvEncoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=31, padding_l=15, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=496, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (layer_norms): ModuleList(\n",
              "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (5): LightConvEncoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=31, padding_l=15, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=496, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (layer_norms): ModuleList(\n",
              "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (6): LightConvEncoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=31, padding_l=15, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=496, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (layer_norms): ModuleList(\n",
              "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (decoder): LightConvDecoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(32595, 1024, padding_idx=1)\n",
              "        (embed_positions): SinusoidalPositionalEmbedding()\n",
              "        (layers): ModuleList(\n",
              "          (0): LightConvDecoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=3, padding_l=2, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=48, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (conv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): LightConvDecoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=7, padding_l=6, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=112, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (conv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): LightConvDecoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=15, padding_l=14, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=240, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (conv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): LightConvDecoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=31, padding_l=30, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=496, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (conv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): LightConvDecoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=31, padding_l=30, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=496, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (conv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): LightConvDecoderLayer(\n",
              "            dropout=0.25, relu_dropout=0.0, input_dropout=0.1, normalize_before=False\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (act): GLU(dim=-1)\n",
              "            (conv): DynamicConv1dTBC(\n",
              "              1024, kernel_size=31, padding_l=30, num_heads=16, weight_softmax=True, conv_bias=False, renorm_padding=False, in_proj=False, weight_dropout=0.25\n",
              "              (weight_dropout_module): FairseqDropout()\n",
              "              (weight_linear): Linear(in_features=1024, out_features=496, bias=False)\n",
              "            )\n",
              "            (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (relu_dropout_module): FairseqDropout()\n",
              "            (input_dropout_module): FairseqDropout()\n",
              "            (conv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkE6m9JFMGIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ebeabae2-e317-45cc-e9aa-d035f268e37f"
      },
      "source": [
        "def process_tcn(line):\n",
        "    zh_line = HanziConv.toSimplified(str(line))\n",
        "    return ' '.join([w for w in jieba.cut(zh_line, cut_all=False) if w.strip() != ''])\n",
        "\n",
        "\n",
        "def read_txt_to_list(fn):\n",
        "    res = []\n",
        "    with open(fn, 'r') as f:\n",
        "        for line in f:\n",
        "            res.append(line.strip())\n",
        "    return res\n",
        "\n",
        "def write_list_to_txt(lines, fn):\n",
        "    with open(fn, 'w') as f:\n",
        "        for line in lines:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "\n",
        "zh_file = 'drive/My Drive/shopee-titles/train_zh.txt'\n",
        "\n",
        "if not os.path.isfile(zh_file):\n",
        "    zh_lines = [process_tcn(line) for line in dfs['train_tcn_cleaned'].product_title.values]\n",
        "    write_list_to_txt(zh_lines, zh_file)\n",
        "else:\n",
        "    zh_lines = read_txt_to_list(zh_file)\n",
        "\n",
        "CHUNKSIZE = 50000\n",
        "n_chunks = len(zh_lines)//CHUNKSIZE + 1\n",
        "print(f'Number of chunks = {n_chunks}')\n",
        "\n",
        "zh_chunk_files = [f'zh_chunk{i}.txt' for i in range(n_chunks)]\n",
        "translate_chunk_files = [f'drive/My Drive/shopee-titles/translated_lightconv_zh_chunk{i}.txt' for i in range(n_chunks)]\n",
        "\n",
        "for i in range(n_chunks):\n",
        "    if not os.path.isfile(zh_chunk_files[i]):\n",
        "        write_list_to_txt(zh_lines[i*CHUNKSIZE:(i+1)*CHUNKSIZE], zh_chunk_files[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.972 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of chunks = 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ4H3-P0iZ8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_zh_lines = [process_tcn(line) for line in dfs['dev_tcn'].text.values]\n",
        "write_list_to_txt(dev_zh_lines, 'dev_zh_lines.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klY1_RDxhi_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88896c49-558f-490f-aa3f-6aa0611ae439"
      },
      "source": [
        "def translate_zh2en(zh_file, translate_file, batchsize=256):\n",
        "    zh_lines = read_txt_to_list(zh_file)\n",
        "    start_time = time.time()\n",
        "    zh_translate_lightconv = []\n",
        "    n_batches = len(zh_lines)//batchsize + 1\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        print('iter #{}/{}, elapsed time: {:.2f}s'.format(i, n_batches, time.time()-start_time))\n",
        "        batch = zh_lines[i*batchsize:(i+1)*batchsize]\n",
        "        translated_batch = zh2en.translate(batch, beam = 8)\n",
        "        zh_translate_lightconv.extend(translated_batch)\n",
        "\n",
        "    write_list_to_txt(zh_translate_lightconv, translate_file)\n",
        "\n",
        "ZH_CHUNKS_TO_TRANSLATE = [6]\n",
        "\n",
        "for i in ZH_CHUNKS_TO_TRANSLATE:\n",
        "    zh_cf, trl_cf = zh_chunk_files[i], translate_chunk_files[i]\n",
        "    if not os.path.isfile(trl_cf):\n",
        "        print('Translate ' + zh_cf)\n",
        "        translate_zh2en(zh_cf, trl_cf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translate zh_chunk6.txt\n",
            "iter #0/196, elapsed time: 0.00s\n",
            "iter #1/196, elapsed time: 7.51s\n",
            "iter #2/196, elapsed time: 14.89s\n",
            "iter #3/196, elapsed time: 27.09s\n",
            "iter #4/196, elapsed time: 35.26s\n",
            "iter #5/196, elapsed time: 44.94s\n",
            "iter #6/196, elapsed time: 51.96s\n",
            "iter #7/196, elapsed time: 61.54s\n",
            "iter #8/196, elapsed time: 70.03s\n",
            "iter #9/196, elapsed time: 82.56s\n",
            "iter #10/196, elapsed time: 92.41s\n",
            "iter #11/196, elapsed time: 101.65s\n",
            "iter #12/196, elapsed time: 109.96s\n",
            "iter #13/196, elapsed time: 119.16s\n",
            "iter #14/196, elapsed time: 127.02s\n",
            "iter #15/196, elapsed time: 138.62s\n",
            "iter #16/196, elapsed time: 146.34s\n",
            "iter #17/196, elapsed time: 156.08s\n",
            "iter #18/196, elapsed time: 165.95s\n",
            "iter #19/196, elapsed time: 174.06s\n",
            "iter #20/196, elapsed time: 181.40s\n",
            "iter #21/196, elapsed time: 190.81s\n",
            "iter #22/196, elapsed time: 198.39s\n",
            "iter #23/196, elapsed time: 207.68s\n",
            "iter #24/196, elapsed time: 217.12s\n",
            "iter #25/196, elapsed time: 227.86s\n",
            "iter #26/196, elapsed time: 237.06s\n",
            "iter #27/196, elapsed time: 246.64s\n",
            "iter #28/196, elapsed time: 254.06s\n",
            "iter #29/196, elapsed time: 261.85s\n",
            "iter #30/196, elapsed time: 271.26s\n",
            "iter #31/196, elapsed time: 280.74s\n",
            "iter #32/196, elapsed time: 289.80s\n",
            "iter #33/196, elapsed time: 297.47s\n",
            "iter #34/196, elapsed time: 304.31s\n",
            "iter #35/196, elapsed time: 313.08s\n",
            "iter #36/196, elapsed time: 321.38s\n",
            "iter #37/196, elapsed time: 330.92s\n",
            "iter #38/196, elapsed time: 339.72s\n",
            "iter #39/196, elapsed time: 348.83s\n",
            "iter #40/196, elapsed time: 360.63s\n",
            "iter #41/196, elapsed time: 368.07s\n",
            "iter #42/196, elapsed time: 376.09s\n",
            "iter #43/196, elapsed time: 385.51s\n",
            "iter #44/196, elapsed time: 392.97s\n",
            "iter #45/196, elapsed time: 402.17s\n",
            "iter #46/196, elapsed time: 410.92s\n",
            "iter #47/196, elapsed time: 420.78s\n",
            "iter #48/196, elapsed time: 429.86s\n",
            "iter #49/196, elapsed time: 437.79s\n",
            "iter #50/196, elapsed time: 445.88s\n",
            "iter #51/196, elapsed time: 455.81s\n",
            "iter #52/196, elapsed time: 463.29s\n",
            "iter #53/196, elapsed time: 474.98s\n",
            "iter #54/196, elapsed time: 485.77s\n",
            "iter #55/196, elapsed time: 492.98s\n",
            "iter #56/196, elapsed time: 501.28s\n",
            "iter #57/196, elapsed time: 513.42s\n",
            "iter #58/196, elapsed time: 523.05s\n",
            "iter #59/196, elapsed time: 532.46s\n",
            "iter #60/196, elapsed time: 541.74s\n",
            "iter #61/196, elapsed time: 549.07s\n",
            "iter #62/196, elapsed time: 557.13s\n",
            "iter #63/196, elapsed time: 568.11s\n",
            "iter #64/196, elapsed time: 575.83s\n",
            "iter #65/196, elapsed time: 583.58s\n",
            "iter #66/196, elapsed time: 591.59s\n",
            "iter #67/196, elapsed time: 600.94s\n",
            "iter #68/196, elapsed time: 611.17s\n",
            "iter #69/196, elapsed time: 618.19s\n",
            "iter #70/196, elapsed time: 627.51s\n",
            "iter #71/196, elapsed time: 637.26s\n",
            "iter #72/196, elapsed time: 647.79s\n",
            "iter #73/196, elapsed time: 654.73s\n",
            "iter #74/196, elapsed time: 664.68s\n",
            "iter #75/196, elapsed time: 673.31s\n",
            "iter #76/196, elapsed time: 682.68s\n",
            "iter #77/196, elapsed time: 690.51s\n",
            "iter #78/196, elapsed time: 700.06s\n",
            "iter #79/196, elapsed time: 707.89s\n",
            "iter #80/196, elapsed time: 718.16s\n",
            "iter #81/196, elapsed time: 726.12s\n",
            "iter #82/196, elapsed time: 735.65s\n",
            "iter #83/196, elapsed time: 743.03s\n",
            "iter #84/196, elapsed time: 752.59s\n",
            "iter #85/196, elapsed time: 759.37s\n",
            "iter #86/196, elapsed time: 768.24s\n",
            "iter #87/196, elapsed time: 777.38s\n",
            "iter #88/196, elapsed time: 784.85s\n",
            "iter #89/196, elapsed time: 792.63s\n",
            "iter #90/196, elapsed time: 800.39s\n",
            "iter #91/196, elapsed time: 810.05s\n",
            "iter #92/196, elapsed time: 817.76s\n",
            "iter #93/196, elapsed time: 825.60s\n",
            "iter #94/196, elapsed time: 834.73s\n",
            "iter #95/196, elapsed time: 844.03s\n",
            "iter #96/196, elapsed time: 851.02s\n",
            "iter #97/196, elapsed time: 859.19s\n",
            "iter #98/196, elapsed time: 868.33s\n",
            "iter #99/196, elapsed time: 876.99s\n",
            "iter #100/196, elapsed time: 884.22s\n",
            "iter #101/196, elapsed time: 893.50s\n",
            "iter #102/196, elapsed time: 902.72s\n",
            "iter #103/196, elapsed time: 911.91s\n",
            "iter #104/196, elapsed time: 919.70s\n",
            "iter #105/196, elapsed time: 928.88s\n",
            "iter #106/196, elapsed time: 938.58s\n",
            "iter #107/196, elapsed time: 948.00s\n",
            "iter #108/196, elapsed time: 957.55s\n",
            "iter #109/196, elapsed time: 969.39s\n",
            "iter #110/196, elapsed time: 978.60s\n",
            "iter #111/196, elapsed time: 988.23s\n",
            "iter #112/196, elapsed time: 997.41s\n",
            "iter #113/196, elapsed time: 1006.57s\n",
            "iter #114/196, elapsed time: 1015.61s\n",
            "iter #115/196, elapsed time: 1025.55s\n",
            "iter #116/196, elapsed time: 1036.21s\n",
            "iter #117/196, elapsed time: 1043.85s\n",
            "iter #118/196, elapsed time: 1054.27s\n",
            "iter #119/196, elapsed time: 1064.35s\n",
            "iter #120/196, elapsed time: 1074.47s\n",
            "iter #121/196, elapsed time: 1083.93s\n",
            "iter #122/196, elapsed time: 1094.80s\n",
            "iter #123/196, elapsed time: 1101.98s\n",
            "iter #124/196, elapsed time: 1111.62s\n",
            "iter #125/196, elapsed time: 1119.59s\n",
            "iter #126/196, elapsed time: 1127.21s\n",
            "iter #127/196, elapsed time: 1134.83s\n",
            "iter #128/196, elapsed time: 1144.72s\n",
            "iter #129/196, elapsed time: 1152.47s\n",
            "iter #130/196, elapsed time: 1159.90s\n",
            "iter #131/196, elapsed time: 1168.81s\n",
            "iter #132/196, elapsed time: 1178.09s\n",
            "iter #133/196, elapsed time: 1187.95s\n",
            "iter #134/196, elapsed time: 1195.08s\n",
            "iter #135/196, elapsed time: 1204.62s\n",
            "iter #136/196, elapsed time: 1212.46s\n",
            "iter #137/196, elapsed time: 1221.25s\n",
            "iter #138/196, elapsed time: 1230.34s\n",
            "iter #139/196, elapsed time: 1237.92s\n",
            "iter #140/196, elapsed time: 1246.78s\n",
            "iter #141/196, elapsed time: 1255.96s\n",
            "iter #142/196, elapsed time: 1264.39s\n",
            "iter #143/196, elapsed time: 1275.14s\n",
            "iter #144/196, elapsed time: 1282.57s\n",
            "iter #145/196, elapsed time: 1292.63s\n",
            "iter #146/196, elapsed time: 1304.27s\n",
            "iter #147/196, elapsed time: 1311.89s\n",
            "iter #148/196, elapsed time: 1318.58s\n",
            "iter #149/196, elapsed time: 1327.31s\n",
            "iter #150/196, elapsed time: 1334.80s\n",
            "iter #151/196, elapsed time: 1345.16s\n",
            "iter #152/196, elapsed time: 1352.92s\n",
            "iter #153/196, elapsed time: 1360.35s\n",
            "iter #154/196, elapsed time: 1367.88s\n",
            "iter #155/196, elapsed time: 1377.59s\n",
            "iter #156/196, elapsed time: 1386.60s\n",
            "iter #157/196, elapsed time: 1396.46s\n",
            "iter #158/196, elapsed time: 1407.58s\n",
            "iter #159/196, elapsed time: 1417.90s\n",
            "iter #160/196, elapsed time: 1425.48s\n",
            "iter #161/196, elapsed time: 1432.97s\n",
            "iter #162/196, elapsed time: 1441.66s\n",
            "iter #163/196, elapsed time: 1451.06s\n",
            "iter #164/196, elapsed time: 1460.92s\n",
            "iter #165/196, elapsed time: 1469.93s\n",
            "iter #166/196, elapsed time: 1478.89s\n",
            "iter #167/196, elapsed time: 1486.50s\n",
            "iter #168/196, elapsed time: 1495.80s\n",
            "iter #169/196, elapsed time: 1503.11s\n",
            "iter #170/196, elapsed time: 1513.19s\n",
            "iter #171/196, elapsed time: 1522.03s\n",
            "iter #172/196, elapsed time: 1532.78s\n",
            "iter #173/196, elapsed time: 1541.96s\n",
            "iter #174/196, elapsed time: 1551.60s\n",
            "iter #175/196, elapsed time: 1561.89s\n",
            "iter #176/196, elapsed time: 1572.15s\n",
            "iter #177/196, elapsed time: 1581.36s\n",
            "iter #178/196, elapsed time: 1590.60s\n",
            "iter #179/196, elapsed time: 1600.04s\n",
            "iter #180/196, elapsed time: 1607.41s\n",
            "iter #181/196, elapsed time: 1616.65s\n",
            "iter #182/196, elapsed time: 1626.05s\n",
            "iter #183/196, elapsed time: 1638.45s\n",
            "iter #184/196, elapsed time: 1646.01s\n",
            "iter #185/196, elapsed time: 1654.91s\n",
            "iter #186/196, elapsed time: 1664.04s\n",
            "iter #187/196, elapsed time: 1671.96s\n",
            "iter #188/196, elapsed time: 1680.88s\n",
            "iter #189/196, elapsed time: 1689.80s\n",
            "iter #190/196, elapsed time: 1697.38s\n",
            "iter #191/196, elapsed time: 1706.77s\n",
            "iter #192/196, elapsed time: 1714.47s\n",
            "iter #193/196, elapsed time: 1723.77s\n",
            "iter #194/196, elapsed time: 1731.65s\n",
            "iter #195/196, elapsed time: 1740.43s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r96WnT3irVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "32622069-d6b8-4266-8636-83231b4ae606"
      },
      "source": [
        "translate_zh2en('dev_zh_lines.txt', 'drive/My Drive/shopee-titles/translated_lightconv_dev_zh.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter #0/4, elapsed time: 0.00s\n",
            "iter #1/4, elapsed time: 5.96s\n",
            "iter #2/4, elapsed time: 11.54s\n",
            "iter #3/4, elapsed time: 17.21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fketAIF5vsJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGS26e3u_f7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 10 train_zh.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5nnpVz7-95x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 10 drive/My\\ Drive/shopee-titles/zh_translate_lightconv.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXm56augGnve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # zh2endyn.translate('小拇指 鞋坊 adidas superstar 80s dlx 金标 贝壳 头 白红 时尚 男女 鞋 adidas 板鞋 时尚 情侣 鞋 adids 男鞋 女鞋 36 - 44', beam=8)\n",
        "dev_tcn = dfs['dev_tcn'].text.values\n",
        "dev_zh = [process_tcn(a) for a in dev_tcn]\n",
        "dev_en = dfs['dev_en'].translation_output.values\n",
        "trans1 = zh2en.translate(dev_zh, beam=16)\n",
        "trans2 = zh2endyn.translate(dev_zh, beam=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVGWJsQRpzCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trans [t1 if len(tcn)>30 else t2 for t1, t2, tcn in zip()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPE7NBzwmeBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "584b53c8-b182-42f5-ac60-ce0fad9d59a6"
      },
      "source": [
        "import re\n",
        "from typing import List\n",
        "\n",
        "import regex\n",
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "OTHERS_PATTERN = regex.compile(r'\\p{So}')\n",
        "\n",
        "\n",
        "def shopee_eval(preds: List[str], refs: List[str]) -> float:\n",
        "    \"\"\"BLEU score computation.\n",
        "\n",
        "    Strips all characters belonging to the unicode category \"So\".\n",
        "    Tokenize with standard WMT \"13a\" tokenizer.\n",
        "    Compute 4-BLEU.\n",
        "\n",
        "    Args:\n",
        "        preds (List[str]): List of translated texts.\n",
        "        refs (List[str]): List of target reference texts.\n",
        "    \"\"\"\n",
        "    preds = [OTHERS_PATTERN.sub(' ', text) for text in preds]\n",
        "    refs = [OTHERS_PATTERN.sub(' ', text) for text in refs]\n",
        "    return corpus_bleu(\n",
        "        preds, [refs],\n",
        "        lowercase=True,\n",
        "        tokenize='13a',\n",
        "        use_effective_order= False\n",
        "    ).score\n",
        "\n",
        "#  ('IFairies Shoulder Messenger Bag Shoulder Bag 【 49146 】',\n",
        "def post_process(line):\n",
        "    line = line.replace('-', ' - ')\n",
        "    line = line.replace('[', ' 】')\n",
        "    line = line.replace('[', '【 ')\n",
        "    line = line.replace('  ', ' ')\n",
        "    line = line.replace('  ', ' ')\n",
        "    return line\n",
        "\n",
        "for beam in range(1,16):\n",
        "    trans1 = [post_process(line) for line in zh2en.translate(dev_zh, beam=beam)]\n",
        "    print(beam, shopee_eval(dev_en, trans1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 29.61639208717485\n",
            "2 29.90893936681329\n",
            "3 30.561801970869652\n",
            "4 30.871153786067573\n",
            "5 30.99717241050713\n",
            "6 31.349587918719866\n",
            "7 31.366437848168182\n",
            "8 31.34761450366679\n",
            "9 31.334267626666584\n",
            "10 31.521912412746744\n",
            "11 31.493725861556264\n",
            "12 31.47342216904188\n",
            "13 31.465449992038828\n",
            "14 31.403130167715947\n",
            "15 31.408236494898258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv7LzPX84POE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2a3a30de-1e2f-4b59-8f01-070d2c6e688f"
      },
      "source": [
        "for beam in range(1,16):\n",
        "    trans1 = [post_process(line) for line in zh2endyn.translate(dev_zh, beam=beam)]\n",
        "    print(beam, shopee_eval(dev_en, trans1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 28.41365535220877\n",
            "2 29.69533191367675\n",
            "3 29.858194696472943\n",
            "4 30.255306163129614\n",
            "5 30.4812345472558\n",
            "6 30.369630813468557\n",
            "7 30.518815229136365\n",
            "8 30.836793572675255\n",
            "9 30.660489372176713\n",
            "10 30.521656301302244\n",
            "11 30.59820362772005\n",
            "12 30.574116675075413\n",
            "13 30.493524479885057\n",
            "14 30.418824329222947\n",
            "15 30.37693679078742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0_S12RDhpxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_zh = [process_tcn(line) for line in dfs['test_tcn'].text.values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeZswYmSYPqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "40639053-006f-4b9e-fed3-d8a30c653b10"
      },
      "source": [
        "write_list_to_txt(test_zh, 'test_zh.txt')\n",
        "translate_zh2en('test_zh.txt', 'test_translated.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter #0/40, elapsed time: 0.00s\n",
            "iter #1/40, elapsed time: 6.09s\n",
            "iter #2/40, elapsed time: 11.80s\n",
            "iter #3/40, elapsed time: 17.52s\n",
            "iter #4/40, elapsed time: 23.13s\n",
            "iter #5/40, elapsed time: 28.73s\n",
            "iter #6/40, elapsed time: 35.31s\n",
            "iter #7/40, elapsed time: 41.14s\n",
            "iter #8/40, elapsed time: 46.84s\n",
            "iter #9/40, elapsed time: 52.36s\n",
            "iter #10/40, elapsed time: 57.99s\n",
            "iter #11/40, elapsed time: 63.74s\n",
            "iter #12/40, elapsed time: 71.82s\n",
            "iter #13/40, elapsed time: 77.48s\n",
            "iter #14/40, elapsed time: 83.06s\n",
            "iter #15/40, elapsed time: 88.41s\n",
            "iter #16/40, elapsed time: 94.36s\n",
            "iter #17/40, elapsed time: 102.38s\n",
            "iter #18/40, elapsed time: 107.70s\n",
            "iter #19/40, elapsed time: 113.19s\n",
            "iter #20/40, elapsed time: 119.07s\n",
            "iter #21/40, elapsed time: 124.69s\n",
            "iter #22/40, elapsed time: 130.34s\n",
            "iter #23/40, elapsed time: 135.87s\n",
            "iter #24/40, elapsed time: 141.82s\n",
            "iter #25/40, elapsed time: 147.29s\n",
            "iter #26/40, elapsed time: 153.18s\n",
            "iter #27/40, elapsed time: 161.24s\n",
            "iter #28/40, elapsed time: 167.26s\n",
            "iter #29/40, elapsed time: 172.83s\n",
            "iter #30/40, elapsed time: 178.15s\n",
            "iter #31/40, elapsed time: 184.07s\n",
            "iter #32/40, elapsed time: 189.74s\n",
            "iter #33/40, elapsed time: 195.45s\n",
            "iter #34/40, elapsed time: 203.21s\n",
            "iter #35/40, elapsed time: 208.96s\n",
            "iter #36/40, elapsed time: 214.85s\n",
            "iter #37/40, elapsed time: 220.11s\n",
            "iter #38/40, elapsed time: 226.62s\n",
            "iter #39/40, elapsed time: 232.86s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I75QpTWrYwHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_translations = read_txt_to_list('test_translated.txt')\n",
        "dev_translations = zh2en.translate(dev_zh, beam=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqYjATxs9NJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_translations_post = []\n",
        "for zhline, transline in zip(test_zh, test_translations):\n",
        "    transline = transline.replace('-', ' - ')\n",
        "    if ']' not in zhline:\n",
        "        transline = transline.replace(']', ' 】')\n",
        "    if '[' not in zhline:\n",
        "        transline = transline.replace('[', '【 ')\n",
        "    transline = transline.replace('  ', ' ')\n",
        "    transline = transline.replace('  ', ' ')\n",
        "    test_translations_post.append(transline)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijawOcn65qKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_translations2 = zh2endyn.translate(dev_zh, beam=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsDKQmWjoBER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_translations_cleaned = [OTHERS_PATTERN.sub(' ', text) for text in test_translations]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21WTXvb9xiqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b75fa83-8bae-4337-b8c0-6400c3e73c0d"
      },
      "source": [
        "shopee_eval(['KAWECO SUpra Series Brass Pen, Germany'], ['KAWECO SUpra Series Brass Pen, Germany'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.00000000000004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr-w_N8t0ZWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_translations_df = pd.DataFrame({'translation_output': test_translations_post})\n",
        "test_translations_df.to_csv('submit.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydfyjdWD7Npn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcEDqBWFaeGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dev_en_cleaned = [OTHERS_PATTERN.sub(' ', text) for text in dev_en]\n",
        "# dev_translations_cleaned = [OTHERS_PATTERN.sub(' ', text) for text in dev_translations]\n",
        "# dev_translations2_cleaned = [OTHERS_PATTERN.sub(' ', text) for text in dev_translations2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7abSDXPPaysH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import time\n",
        "# import googletrans\n",
        "# from googletrans import Translator\n",
        "# googletrans.LANGUAGES\n",
        "\n",
        "# translator = Translator(service_urls=[\n",
        "#     'translate.google.com',\n",
        "#     'translate.google.com.vn'\n",
        "# ])\n",
        "\n",
        "# all_translations = []\n",
        "# for i in range(50):\n",
        "#     print(i)\n",
        "#     batch = test_sentences[i*200:(i+1)*200]\n",
        "#     translations = translator.translate(batch.tolist(), dest='en', src='zh-tw')\n",
        "#     for t in translations:\n",
        "#         all_translations.append(t.text)\n",
        "#     time.sleep(5)\n",
        "\n",
        "# with open('/content/drive/My Drive/shopee-titles/shopee_googletranslate.csv', 'w') as f:\n",
        "#     for line in all_translations:\n",
        "#         f.write(line + '\\n')\n",
        "# with open('shopee_googletranslate.csv', 'w') as f:\n",
        "#     for line in all_translations:\n",
        "#         f.write(line + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjMxFacyPLuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install mstranslate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjpswMXJPN0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os, requests, uuid, json\n",
        "# # key_var_name = 'TRANSLATOR_TEXT_SUBSCRIPTION_KEY'\n",
        "# # if not key_var_name in os.environ:\n",
        "# #     raise Exception('Please set/export the environment variable: {}'.format(key_var_name))\n",
        "# subscription_key = 'b1295591752c41a49e1f765364ac9282'\n",
        "\n",
        "# # endpoint_var_name = 'TRANSLATOR_TEXT_ENDPOINT'\n",
        "# # if not endpoint_var_name in os.environ:\n",
        "# #     raise Exception('Please set/export the environment variable: {}'.format(endpoint_var_name))\n",
        "# endpoint = 'https://api.cognitive.microsofttranslator.com/'\n",
        "\n",
        "# path = '/translate?api-version=3.0'\n",
        "# params = '&to=en'\n",
        "# constructed_url = endpoint + path + params\n",
        "# headers = {\n",
        "#     'Ocp-Apim-Subscription-Key': subscription_key,\n",
        "#     'Content-type': 'application/json',\n",
        "#     'X-ClientTraceId': str(uuid.uuid4())\n",
        "# }\n",
        "# body = [{\n",
        "#     'text': '兒童套裝 台灣製薄長袖居家套裝 魔法Baby~k60092\t!'\n",
        "# }]\n",
        "# request = requests.post(constructed_url, headers=headers, json=body)\n",
        "# response = request.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ndrZKtiw47B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# titles = dfs['train_tcn_cleaned'].product_title.values\n",
        "# batches = []\n",
        "# for i in range(11):\n",
        "#     batch = titles[i*50000:(i+1)*50000]\n",
        "#     batches.append(batch)\n",
        "\n",
        "# for i in range(11):\n",
        "#     with open(f'train_tcn_cleaned_batch{i}.txt', 'w') as f:\n",
        "#         for line in batches[i]:\n",
        "#             f.write(line + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq8SA01fS0OG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}